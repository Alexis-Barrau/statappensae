{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba18eaf6-e231-4086-aa2f-a62a2d331d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/mamba/lib/python3.10/site-packages (2.1.4)\n",
      "Collecting pyreadr\n",
      "  Downloading pyreadr-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/mamba/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/mamba/lib/python3.10/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pyreadr-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.9/440.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyreadr\n",
      "Successfully installed pyreadr-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyreadr\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f58bd8a-c51d-4b09-8d85-c5b0e5c4244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des fichiers csv\n",
    "def extract_strings_from_webpage(url):\n",
    "    response = requests.get(url) \n",
    "    if response.status_code == 200:\n",
    "        strings = re.findall(r'\"([^\"]*)\"', response.text)\n",
    "        return strings\n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "webpage_url = \"https://unehistoireduconflitpolitique.fr/telecharger.html\"\n",
    "extracted_strings = extract_strings_from_webpage(webpage_url)\n",
    "\n",
    "download_links = [item for item in extracted_strings if item.endswith(\"csv.zip\") or item.endswith(\"csp.zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03efe9a8-a9a8-4a43-b416-bf4bb180f28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 67/67 [00:34<00:00,  1.95file/s]\n"
     ]
    }
   ],
   "source": [
    "# Téléchargement des fichiers\n",
    "os.makedirs('data_zip', exist_ok=True)\n",
    "\n",
    "progress_bar = tqdm(total=len(download_links), desc=\"Downloading\", unit=\"file\")\n",
    "\n",
    "for link in download_links:\n",
    "    try:\n",
    "        file_name = os.path.join('data_zip', os.path.basename(link))\n",
    "        response = requests.get(link)\n",
    "        with open(file_name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        progress_bar.update(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {link}: {e}\")\n",
    "        \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b07636-d58e-4919-a226-aa50d18d9008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 57/57 [00:12<00:00,  4.70file/s]\n"
     ]
    }
   ],
   "source": [
    "# Extraction des résultats électoraux\n",
    "for prefix in ['pres', 'leg', 'ref']:\n",
    "    prefix_dir = os.path.join('data_csv', 'Elections_' + prefix)\n",
    "    os.makedirs(prefix_dir, exist_ok=True)\n",
    "\n",
    "zip_files = [file for file in os.listdir('data_zip') if file.endswith('.zip')]\n",
    "total_zip_files = sum(file.startswith(prefix) for prefix in ['pres', 'leg', 'ref'] for file in zip_files)\n",
    "progress_bar = tqdm(total=total_zip_files, desc=\"Extracting\", unit=\"file\")\n",
    "\n",
    "for prefix in ['pres', 'leg', 'ref']:\n",
    "    for file in zip_files:\n",
    "        if file.startswith(prefix) and file.endswith('.zip'):\n",
    "            try:\n",
    "                zip_file_path = os.path.join('data_zip', file)\n",
    "                prefix_dir = os.path.join('data_csv', 'Elections_' + prefix)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    for member in zip_ref.infolist():\n",
    "                        if member.filename.lower().endswith('.csv'):\n",
    "                            target_path = os.path.join(prefix_dir, os.path.basename(member.filename))\n",
    "                            with zip_ref.open(member) as source, open(target_path, 'wb') as dest:\n",
    "                                shutil.copyfileobj(source, dest)\n",
    "                os.remove(zip_file_path)\n",
    "                progress_bar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {file}: {e}\")\n",
    "            \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6485778-11b9-4780-9a7b-b47562db7428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion of extra files completed.\n"
     ]
    }
   ],
   "source": [
    "# Nettoyage des résultats électoraux\n",
    "for root, dirs, files in os.walk('data_csv'):\n",
    "    for file_name in files:\n",
    "        if file_name.startswith(\"._\"):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file_path}: {e}\")\n",
    "print(\"Deletion of extra files completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55488d2-9d45-49a1-8603-0ffd67b664a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 10/10 [00:51<00:00,  5.15s/file]\n"
     ]
    }
   ],
   "source": [
    "# Extraction des contrôles\n",
    "zip_files = [f for f in os.listdir('data_zip') if f.endswith('.zip')]\n",
    "total_zip_files = sum(not any(file.startswith(prefix) for prefix in ['pres', 'leg', 'ref']) for file in zip_files)\n",
    "progress_bar = tqdm(total=total_zip_files, desc=\"Extracting\", unit=\"file\")\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    if zip_file.startswith(('pres', 'leg', 'ref')):\n",
    "        continue\n",
    "    zip_path = os.path.join('data_zip', zip_file)\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            file_list = [file for file in zip_ref.namelist() if not file.startswith('__MACOSX')]\n",
    "            zip_ref.extractall('data_csv', members=file_list)\n",
    "            os.remove(zip_path)            \n",
    "            progress_bar.update(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {zip_file}: {e}\")\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029b864d-a390-4f9f-8c3b-70b8d9fdeb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# Nettoyage des contrôles\n",
    "folders = [f for f in os.listdir('data_csv') if os.path.isdir(os.path.join('data_csv', f))]\n",
    "for folder in folders:\n",
    "    if folder.endswith('_csv'):\n",
    "        old_path = os.path.join('data_csv', folder)\n",
    "        new_folder_name = folder[:-4]\n",
    "        new_path = os.path.join('data_csv', new_folder_name)\n",
    "        os.rename(old_path, new_path)\n",
    "\n",
    "shutil.move('data_csv/alphabetisationcommunes.csv', 'data_csv/Diplomes/')\n",
    "\n",
    "print(\"Folder cleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42722a4a-81b5-4626-830d-eddb31cb3ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data removed.\n"
     ]
    }
   ],
   "source": [
    "# Suppression du répertoire de téléchargement\n",
    "shutil.rmtree('data_zip')\n",
    "print('Downloaded data removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5d4c15-51fc-446e-8830-576eb1fb87de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting:  60%|██████    | 66/110 [32:22<21:35, 29.44s/file]\n",
      "\n",
      "Converting:   2%|▏         | 1/45 [00:05<04:00,  5.47s/file]\u001b[A"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x9e in position 171: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m output_rda_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_rda\u001b[39m\u001b[38;5;124m'\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(relative_path)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.rda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(output_rda_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m pyreadr\u001b[38;5;241m.\u001b[39mwrite_rdata(output_rda_path, data, compress\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(input_csv_path)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x9e in position 171: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Conversion au format rda\n",
    "total_csv_files = 0\n",
    "for root, dirs, files in os.walk('data_csv'):\n",
    "    csv_files = [file for file in files if file.endswith(\".csv\")]\n",
    "    total_csv_files += len(csv_files)\n",
    "\n",
    "progress_bar = tqdm(total=total_csv_files, desc=\"Converting\", unit=\"file\")\n",
    "\n",
    "for root, dirs, files in os.walk('data_csv'):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            input_csv_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(input_csv_path, 'data_csv')\n",
    "            output_rda_path = os.path.join('data_rda', os.path.splitext(relative_path)[0] + \".rda\")\n",
    "            os.makedirs(os.path.dirname(output_rda_path), exist_ok=True)\n",
    "            data = pd.read_csv(input_csv_path, low_memory=False)\n",
    "            pyreadr.write_rdata(output_rda_path, data, compress='gzip')\n",
    "            os.remove(input_csv_path)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df973806-fe31-4dd5-aeac-7e7c69b2ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supression du répertoire d'extraction\n",
    "shutil.rmtree('data_csv')\n",
    "print('Extracted data removed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
